## Put your model args here
gmm_params:
    K: 5
    D: 3

is_typed: ${if:${eq:${data.cat_strategy},uniform-sample},true,false}
output_dim_typed: ${oc.eval:"${model.gmm_params.K} * (2 + ${model.gmm_params.D} + ${len:${data.vocab.atom_tokens}} + ${len:${data.vocab.charge_tokens}})"}
output_dim_untyped: ${oc.eval:"${model.gmm_params.K} * (3 +  ${model.gmm_params.D})"}

integrator:
  _target_: chemflow.flow_matching.integration.RateIntegrator
  vocab: ${data.vocab}
  distributions: None
  gmm_params: ${model.gmm_params}
  cat_strategy: ${data.cat_strategy}
  n_atoms_strategy: ${data.n_atoms_strategy}
  time_strategy: log # "linear" or "log"
  num_integration_steps: 100


module:
  _target_: chemflow.model.lightning_module.LightningModuleRates
  # args and kwargs 
  # tokens and atom_type_distribution will be passed from preprocessing in run.py
  gmm_params: ${model.gmm_params}
  n_atoms_strategy: ${data.n_atoms_strategy}
  type_loss_token_weights: training
  cat_weighting:
    _target_: DictConfig
    cat_strategy: ${data.cat_strategy}
    weight_alpha: 1.0
  noise_params:
    _target_: DictConfig
    cat_noise_level: 0.1
    coord_noise_level: 0.0
  time_dist:
    # Beta distribution for time scheduling. 1.0 , 1.0 is equivalent to uniform distribution.
    # SEMLAFLOW uses 2.0, 1.0
    _target_: torch.distributions.Beta
    concentration1: 2.0
    concentration0: 1.0
  integrator: ${model.integrator}

  vocab: ${data.vocab}
  
  # Loss component weights
  loss_weights:
    # Flow matching loss weights
    l_x: 20.0
    # EditFlow loss weights
    # Substitution is used for atom type changes
    l_sub_rate_a: 1.0
    l_sub_class_a: 1.0

    l_sub_rate_c: 1.0
    l_sub_class_c: 1.0

    l_sub_rate_e: 1.0
    l_sub_class_e: 1.0
    
    # Only use deletion and insertion losses if the number of atoms is flexible
    l_ins: ${if:${eq:${data.n_atoms_strategy},flexible},1.0,0.0}
    l_del: ${if:${eq:${data.n_atoms_strategy},flexible},1.0,0.0}

  # Optimizer configuration
  optimizer_config:
    optimizer:
      _target_: torch.optim.AdamW
      lr: 5e-4
      betas: [0.9, 0.999]
      weight_decay: 1e-2
      fused: true

    scheduler:
      # _target_: chemflow.schedulers.LinearOneCycleLR
      # max_lr: 1e-3
      # warmup_steps: 1000
      _target_: chemflow.schedulers.CosineWarmupLR
      max_lr: 1e-3
      min_lr: 1e-5
      warmup_steps: 1000
      total_steps: 100000

    interval: step
    monitor: null

  # The 'model' key below is an argument to your LightningModule
  model:
    _target_: chemflow.model.gnn.EGNNwithHeads
    _recursive_: false

    atom_type_embedding_args: ${model.atom_type_embedding}
    edge_type_embedding_args: ${model.edge_type_embedding}
    charge_embedding_args: ${model.charge_embedding}
    time_embedding_args: ${model.time_embedding}
    node_count_embedding_args: ${model.node_count_embedding}
    egnn_args: ${model.egnn}
    heads_args: ${model.heads}
    gmm_head_args: ${model.gmm_head_args}
    self_conditioning: False
    ins_edge_head_args:
      _target_: chemflow.model.heads.InsertionEdgeHead
      _recursive_: false
      hidden_dim: 128  # Match your EGNN hidden dim
      n_edge_types: ${len:${data.vocab.edge_tokens}} # Your number of edge types
      n_atom_types: ${len:${data.vocab.atom_tokens}}  # Your number of atom types
      n_charge_types: ${len:${data.vocab.charge_tokens}}  # Your number of charge types
      rbf_embedding_args:
        _target_: chemflow.model.embedding.RBFEmbedding
        num_rbf: 64
        rbf_dmax: 15.0
        out_dim: 64
        dropout: 0.0

atom_type_embedding:
  _target_: chemflow.model.embedding.Embedding
  num_embeddings: ${len:${data.vocab.atom_tokens}}
  embedding_dim: 64
  out_dim: 64

edge_type_embedding:
  _target_: chemflow.model.embedding.Embedding
  num_embeddings: ${len:${data.vocab.edge_tokens}}
  embedding_dim: 64
  out_dim: 64

charge_embedding:
  _target_: chemflow.model.embedding.Embedding
  num_embeddings: ${len:${data.vocab.charge_tokens}}
  embedding_dim: 64
  out_dim: 64

time_embedding:
  _target_: chemflow.model.embedding.TimeEmbedding
  embedding_dim: 64
  out_dim: 64

node_count_embedding:
  _target_: chemflow.model.embedding.CountEmbedding
  embedding_dim: 64
  out_dim: 64

egnn:
  _target_: chemflow.model.gnn.EGNNWithEdgeType
  _recursive_: false
  # Concatenated embeddings: atom + time + count + charge
  in_node_nf: ${oc.eval:"${model.atom_type_embedding.out_dim} + ${model.charge_embedding.out_dim} + ${model.time_embedding.out_dim} + ${model.node_count_embedding.out_dim}"}
  hidden_nf: 256
  out_node_nf: 128
  in_edge_nf: ${model.edge_type_embedding.out_dim} # set to zero when not predicting edge types
  n_layers: 6
  residual: True
  attention: True
  normalize: True
  tanh: True
  rbf_embedding_args:
    _target_: chemflow.model.embedding.RBFEmbedding
    num_rbf: 64
    rbf_dmax: 15.0
    out_dim: 64
    dropout: 0.0

heads:
  _target_: chemflow.model.heads.MultiHeadModule

  heads_configs:
    # Aggregation method for all graph heads
    graph_aggregation: mean  # Options: mean, sum, max
    
    node_heads:
      atom_type_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: ${len:${data.vocab.atom_tokens}}
        hidden_dim: 64

      charge_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: ${len:${data.vocab.charge_tokens}}
        hidden_dim: 64

      ins_rate_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: 1
        hidden_dim: 64

      del_rate_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: 1
        hidden_dim: 64

      sub_rate_a_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: 1
        hidden_dim: 64
      
      sub_rate_c_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: 1
        hidden_dim: 64

    edge_heads:
      sub_rate_e_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: 1
        hidden_dim: 64

      edge_type_head:
        input_dim: ${model.egnn.out_node_nf}
        output_dim: ${len:${data.vocab.edge_tokens}}
        hidden_dim: 64

gmm_head_args:
  _target_: chemflow.model.heads.EquivariantGMMHead
  _recursive_: false
  hidden_dim: ${model.egnn.out_node_nf}
  K: ${model.gmm_params.K}
  N_a: ${len:${data.vocab.atom_tokens}}
  N_c: ${len:${data.vocab.charge_tokens}}
  rbf_embedding_args: ${model.egnn.rbf_embedding_args}
      
